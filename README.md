# ğŸ° Multi-Armed Bandit Simulation

This project implements a multi-armed bandit solution inspired by **Chapter 2.2: Action-Value Methods** (Figure 2.1) from Sutton & Barto's *Reinforcement Learning: An Introduction (2nd Edition, page 35)*. It explores the effects of different Îµ-greedy strategies on agent performance in a stochastic reward setting.

---

## ğŸ¯ Objective

To implement a stochastic n-armed bandit environment and evaluate Îµ-greedy action selection strategies (Îµ = 0, 0.01, 0.1). The experiment analyzes:

- Average reward over time
- Percentage of optimal action selection

This setup is **based on** the foundational example presented in Sutton & Barto's Figure 2.1.

---

## ğŸ§  Approach

- **Algorithm**: Îµ-greedy action selection  
- **Problem Type**: Stochastic n-armed bandit  
- **Exploration Parameters**: Tested with Îµ = 0, 0.01, and 0.1  
- **Metrics**:
  - Percentage of optimal actions taken
  - Average reward per step  
- **Plays**: 2000 plays over 1000 steps  
- **Implementation**: NumPy-based simulation

---

## ğŸ“Š Results

![Multi-Armed Bandit Results](bandit_results.png)

- Left: % Optimal Action chosen over time  
- Right: Average reward earned over time  
- Îµ = 0.1 performs best due to effective exploration

---

## ğŸ—‚ï¸ Project Structure

```
multi-armed-bandit/
â”œâ”€â”€ multi_armed_bandit.ipynb # Main notebook with bandit simulation and plots
â”œâ”€â”€ bandit_results.png # Combined plot of reward and optimal action
â”œâ”€â”€ README.md # Project overview
```
---

## ğŸ“š References

- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.).  
  Chapter 2.2 â€“ Action-Value Methods, Figure 2.1, Page 35  
  [Read Online (Free PDF)](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)

- NumPy Documentation. (n.d.). [https://numpy.org/doc/](https://numpy.org/doc/)
